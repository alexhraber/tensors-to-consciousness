ðŸŽ¯ OPTIMIZATION THEORY
==================================================

--- Gradient Descent Dynamics ---
Starting at x = 10.000000, f(x) = 106.000000
Step 1: x = 7.990000, f(x) = 69.639099
Step 6: x = 2.584547, f(x) = 11.938339
Step 11: x = 0.813288, f(x) = 5.742767
Step 16: x = 0.232882, f(x) = 5.077522

--- Momentum Methods ---
Vanilla GD final: x = 0.001795
Momentum GD final: x = -0.720136

--- Adaptive Optimization (Adam concept) ---
Adam-style optimization:
Step 2: x = 9.997999, f(x) = 105.959793
Step 4: x = 9.995998, f(x) = 105.919594
Step 6: x = 9.993998, f(x) = 105.879387
Step 8: x = 9.991997, f(x) = 105.839203
Step 10: x = 9.989997, f(x) = 105.799042
